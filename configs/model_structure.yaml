#---------------------------------#
#       Hyper Parameters        
#---------------------------------#
K_NEIGHBOR: 2
SINKHORN_ITERS: 8
BT_DIST_MASK: False
DIST_THRESH: 300


#---------------------------------#
#     Model Structure
#   1. Node_Encoder
#   2. Edge_Encoder
#   3. Static_Graph_Model
#   4. Dynamic_Graph_Model
#   5. Fuse_Model
#---------------------------------#

#---------------------------------#
node_encoder:
  backbone: 'densenet121'
  wight_path: null
  dims_list: [1024,512,256,128,64,32]
  layer_type: 'linear'
  layer_bias: False
  use_batchnorm: True
  activate_func: 'relu'
  lrelu_slope: 0.0


#---------------------------------#
edge_encoder:
#---------------------------------#
#  ImgNorm4 SrcNorm4 TgtNorm4 MeanSizeNorm4 
#  MeanHeightNorm4 MeanWidthNorm4 CorverxNorm4 MaxNorm4 
#  IOUd5 IOU5  GIOUd5 GIOU5 
#  DIOUd5 DIOU5  DIOU-Cos6 IouFamily8
#---------------------------------#
  edge_type: 'DIOUd5'
  dims_list: [5,16,16]
  layer_type: 'linear'
  layer_bias: False
  use_batchnorm: True
  activate_func: 'relu'
  lrelu_slope: 0.0
  # static graph construction settings
  bt_cosine: False 
  bt_self_loop: False
  bt_directed: True

#---------------------------------#
#   3. Static_Graph_Model
# There are three GCN , and the dimension of each layer
# is defined in the following list.
#---------------------------------#
static_graph_model:
#-----------GraphConv-------------#
  layer:
  # [msg_in,msg_out,update_out]
  # msg_in equals to (node emb size *2  + edge emb size)
    - [80,72,64,64]      # 48 + 32   // midchannel - (80 + 64)  / 2 
    - [144,120,96,96]    # 80 + 64   // midchannel - (144 + 96) / 2
    - [208,178,128,128]  # 112 + 96  // midchannel - (208 + 128) / 2 
  aggr: 'max'
  layer_type: 'linear'
  layer_bias: False
  use_batchnorm: True
  activate_func: 'lrelu'
  lrelu_slope: 0.1

#---------------------------------#
dynamic_graph_model:
#----------DoubleEdgeEmb----------#
  layer:
  # [msg_in,msg_mid,msg_out]
  # and msg_in equals to (node emb size *2) + edge emb
    - [144,120,96,96]     # 128 + 16 // midchannel - (144+120) /2
    - [208,168,128,128]   # 192 + 16 // midchannel - (208+128) /2
  aggr: 'max'
  # module layer settings 
  layer_type: 'linear'
  layer_bias: False
  use_batchnorm: True
  activate_func: 'lrelu'
  lrelu_slope: 0.1
  # dynamic graph construction settings
  bt_cosine: False 
  bt_self_loop: False
  bt_directed: True

#---------------------------------#
fuse_model:
  # [in_dim , mid_dim, out_dim]
  fuse1_dims: [512,1024]
  fuse2_dims: [1536,768,384,192] 
  layer_type: 'Conv1d'
  layer_bias: False
  use_batchnorm: True
  activate_func: 'lrelu'
  lrelu_slope: 0.2